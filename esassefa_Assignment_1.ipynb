{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "esassefa_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "m_R4z-lmbSjo"
      },
      "source": [
        "<h2>Assignment 1 - Linear Regression on Boston Housing Data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WJ3cJxt3bSjp"
      },
      "source": [
        "# Elshaday Assefa\n",
        "#The modules we're going to use\n",
        "from __future__ import print_function\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# When you execute a code to plot with a simple SHIFT-ENTER, the plot will be shown directly under the code cell\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Q3ISkzl5yK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a6c365e-84c0-452c-a691-a73e574ae746"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxy4JF9JmSXj"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQ4E_J4sCZH"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Ucal_btMiG"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEMHYXcstqXX"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4SlXpPuUoN"
      },
      "source": [
        "!pip list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VXXIgKFJbSjr"
      },
      "source": [
        "# Load data from scikit-learn, which returns (data, target)\n",
        "# note: if you call \"boston = load_boston()\", it returns a dictionary-like object\n",
        "data, target = datasets.load_boston(True)\n",
        "#preprocessed\n",
        "#print(target.shape)\n",
        "\n",
        "\n",
        "# Split the data into two parts: training data and testing data\n",
        "train_data,test_data,train_target,test_target = train_test_split(data,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NA3ezrA-bSjt"
      },
      "source": [
        "<h4>Use scikit-learn library in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hlqomvfpbSjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "aab9ef4c-07ba-487a-c1ab-9fbd306b2587"
      },
      "source": [
        "# Task 1-1: use linear regression in sklearn\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(train_data, train_target)\n",
        "\n",
        "# Task 1-2: show intercept and coefficents\n",
        "\n",
        "print('intercept: ')\n",
        "print(model.intercept_)\n",
        "print('\\ncoefficients: ')\n",
        "print(model.coef_)\n",
        "\n",
        "\n",
        "# Task 1-3: show errors on training dataset and testing dataset\n",
        "predict1 = model.predict(train_data) #predict y values from given data\n",
        "\n",
        "#plt.scatter(train_target, predict1)\n",
        "mse1 = metrics.mean_squared_error(train_target, predict1, squared = False) #determine mse error from predicted y values against target\n",
        "print('\\nmse for training dataset:')\n",
        "print(mse1)\n",
        "\n",
        "print('\\n')\n",
        "predict2 = model.predict(test_data)\n",
        "\n",
        "\n",
        "\n",
        "#MSE\n",
        "mse2 = metrics.mean_squared_error(test_target, predict2, squared = False)\n",
        "print('mse for testing dataset:')\n",
        "print(mse2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intercept: \n",
            "[30.24675099]\n",
            "\n",
            "coefficients: \n",
            "[[-1.13055924e-01  3.01104641e-02  4.03807204e-02  2.78443820e+00\n",
            "  -1.72026334e+01  4.43883520e+00 -6.29636221e-03 -1.44786537e+00\n",
            "   2.62429736e-01 -1.06467863e-02 -9.15456240e-01  1.23513347e-02\n",
            "  -5.08571424e-01]]\n",
            "\n",
            "mse for training dataset:\n",
            "4.652033184880168\n",
            "\n",
            "\n",
            "mse for testing dataset:\n",
            "4.9286021826653466\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "RpX2tOa5bSjv"
      },
      "source": [
        "<h4>Use analytical solution (normal equation) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YZJJ0sUGbSjv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2910f6a3-90d8-48ae-c7dd-328b81910541"
      },
      "source": [
        "# Task 2-1: Implement a function solving normal equation \n",
        "# Inputs: Training data and  training label\n",
        "# Output: Weights\n",
        "def myNormalEqualFun(X,y):\n",
        "    w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) #function solving normal equation\n",
        "    #print(w.shape)\n",
        "    #print(w)\n",
        "   # Ignore:   m = np.zeros(X.shape) this is a test for the gradient function\n",
        "   # print(m.shape)\n",
        "    return w\n",
        "    \n",
        "# Task 2-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return np.dot(X,w)#dot product of weights and training data\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "\n",
        "# Here we call myNormalEqual to train the model and get weights\n",
        "\n",
        "w = myNormalEqualFun(train_data_intercept,train_target)\n",
        "\n",
        "\n",
        "\n",
        "# Task 2-3: show intercept and coefficents\n",
        "#COME BACK HERE!!!!!\n",
        "print('intercept:')\n",
        "print(w[0])\n",
        "print('coefficients')\n",
        "print(w[1:len(w)-1])\n",
        "\n",
        "\n",
        "\n",
        "# Task 2-4: show errors on training dataset and testing dataset\n",
        "train_prediction = myPredictFun(train_data_intercept, w) #y prediction using updated weights and train data\n",
        "test_prediction = myPredictFun(test_data_intercept, w)#y prediction using updated weights and test data\n",
        "\n",
        "print('mse for training dataset: ')\n",
        "norm_mse1 = metrics.mean_squared_error(train_target, train_prediction, squared = False)\n",
        "print (norm_mse1)\n",
        "\n",
        "print('\\nmse for testing dataset: ')\n",
        "norm_mse2 = metrics.mean_squared_error(test_target, test_prediction, squared = False)\n",
        "print(norm_mse2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intercept:\n",
            "[30.24675099]\n",
            "coefficients\n",
            "[[-1.13055924e-01]\n",
            " [ 3.01104641e-02]\n",
            " [ 4.03807204e-02]\n",
            " [ 2.78443820e+00]\n",
            " [-1.72026334e+01]\n",
            " [ 4.43883520e+00]\n",
            " [-6.29636221e-03]\n",
            " [-1.44786537e+00]\n",
            " [ 2.62429736e-01]\n",
            " [-1.06467863e-02]\n",
            " [-9.15456240e-01]\n",
            " [ 1.23513347e-02]]\n",
            "mse for training dataset: \n",
            "4.6520331848801675\n",
            "\n",
            "mse for testing dataset: \n",
            "4.928602182665711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1hhVEfixbSjx"
      },
      "source": [
        "<h4>Use numerical solution (basic gradient descent) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3H1IxOBubSjy"
      },
      "source": [
        "# Feature scaling\n",
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "\n",
        "# Task 3-1: Implement a function performing gradient descent\n",
        "# Inputs: Training data, training label, learning rate, number of iterations\n",
        "# Output: the final Weights\n",
        "#         the loss history along iterations\n",
        "def myGradientDescentFun(X,y,learning_rate,numItrs, X_test, y_test):\n",
        "  \n",
        "  \n",
        "  w = np.zeros(X.shape[1])# creates one dimensional array [number of features + bias]\n",
        "  #w=np.random.rand(X.shape[1])\n",
        " \n",
        "  w2 = w.reshape(-1,1) #vector to array\n",
        "  \n",
        "  \n",
        "  n = X.shape[0] \n",
        " \n",
        "  iter = 0\n",
        "  #print(w2.shape)\n",
        "  losses = []\n",
        "  mse = 0.0\n",
        "  epochs = []\n",
        "  losses_test=[]\n",
        " \n",
        "  \n",
        "  for i in range(numItrs ) : \n",
        "  \n",
        "    y_prediction = myPredictFun(X,w2) #predict y values for weights before update\n",
        "    ytest_prediction = myPredictFun(X_test, w2)\n",
        "   \n",
        "    er = y_prediction-y #use prediction to find error\n",
        "    g = np.matmul(np.transpose(X),er)/float(n) #calculate gradient\n",
        "    #g = -2/n * X.T.dot(er)\n",
        "    #g = learning_rate*(np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y))\n",
        "   \n",
        "    \n",
        "    w2 = w2 - learning_rate*g #update weights\n",
        "   \n",
        "  \n",
        "    #train_prediction = myPredictFun( X, w2)\n",
        "    \n",
        "    \n",
        "   \n",
        "  \n",
        "    \n",
        "\n",
        "    last = mse\n",
        "    \n",
        "    \n",
        "    mse = np.sqrt(np.square(np.subtract(y, y_prediction)).mean()) #squared error of the difference btw y and y predict\n",
        "    \n",
        "\n",
        "  \n",
        "    mse_test = np.sqrt(np.square(np.subtract(y_test,ytest_prediction)).mean()) #above but for testing data\n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "    if ((mse > last) and (iter >1)): #find iteration that locates global minimum\n",
        "      print('number of iterations: ')\n",
        "      print(iter)\n",
        "      break\n",
        "\n",
        "    losses.append(mse)\n",
        "    losses_test.append(mse_test)\n",
        "    iter = iter +1\n",
        "    epochs.append(iter)\n",
        "    \n",
        "    print(iter)\n",
        "    plt.plot(epochs,losses, 'g',label = 'Training Error' )\n",
        "    plt.plot(epochs, losses_test,'b', label = 'Testing Error' )\n",
        "    plt.legend()\n",
        "    plt.title('Cost Function')\n",
        "    plt.xlabel('# of iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "    print('-----------------------------------')\n",
        "    \n",
        "  return w2, losses, losses_test, epochs\n",
        "\n",
        "# Task 3-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return np.matmul(X,w) #dot product of weights and training data\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "\n",
        "# Here we call myGradientDescentFun to train the model and get weights\n",
        "# Note: you need to figure out good learning rate value and the number of iterations\n",
        "w, loss,loss_test, epochs = myGradientDescentFun(train_data_intercept,train_target,.05,300, test_data_intercept, test_target)\n",
        "#print(w)\n",
        "#print(loss)\n",
        "\n",
        "# Task 3-3: show intercept and coefficents\n",
        "print('intercept:')\n",
        "print(w[0])\n",
        "print('coefficients')\n",
        "print(w[1:len(w)-1])\n",
        "\n",
        "\n",
        "# Task 3-4: show errors on training dataset and testing dataset\n",
        "train_prediction = myPredictFun(train_data_intercept, w)\n",
        "print('Training Error:')\n",
        "print(np.sqrt(np.square(np.subtract(train_target, train_prediction)).mean()))\n",
        "\n",
        "test_prediction = myPredictFun(test_data_intercept, w)\n",
        "print('Testing Error:')\n",
        "print(np.sqrt(np.square(np.subtract(test_target, test_prediction)).mean()))\n",
        "\n",
        "# Task 3-5: plot learning curves showing training errors and testing errors along iterations\n",
        "\n",
        "plt.plot(epochs,loss,'g',label = 'Training Error')\n",
        "plt.plot(epochs, loss_test, 'b',label = 'Testing Error')\n",
        "plt.legend()\n",
        "plt.title('Cost Function')\n",
        "plt.xlabel('# of iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx1UOqmfcNe0"
      },
      "source": [
        "<h4>Use numerical solution (stochastic gradient descent) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96_WVyVKcNss"
      },
      "source": [
        "# Feature scaling\n",
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "\n",
        "# Task 4-1: Implement a function performing gradient descent\n",
        "# Inputs: Training data, training label, leaerning rate, number of epoches, batch size\n",
        "# Output: the final Weights\n",
        "#         the loss history along batches\n",
        "def myGradientDescentFun(X,y,learning_rate,epoches, batchsize, X_test, y_test):\n",
        "    train_size = X.shape[0]\n",
        "    test_size = X_test.shape[0] #testing data sizes\n",
        "    train_batches = int(train_size/batchsize) +1#finding number of batches to iterate through\n",
        "    test_batches = int(test_size/batchsize) +1 \n",
        "    m = batchsize #chose batchsize\n",
        "    train_leftover = train_size % m #last batch doesnt have the same ammount of data sets as previous\n",
        "    print(batchsize)\n",
        "    w = np.zeros(X.shape[1])# creates one dimensional array [number of features + bias]\n",
        "    w2 = w.reshape(-1,1) #array to vector\n",
        "  \n",
        "    \n",
        "    loss = []\n",
        "    mse = 0.0\n",
        "    epochs = []\n",
        "    loss_test=[]\n",
        "    iter = 0\n",
        "    \n",
        "    for i in range(epoches) :\n",
        "      for j in range(train_batches):\n",
        "        if j!= train_batches: #condition: all batches except last batch\n",
        "          y_prediction = myPredictFun(X[(j*m):((j+1)*m)],w2)\n",
        "          y_prediction_all = myPredictFun(X,w2) #predict y against whole training data set\n",
        "          ytest_prediction_all = myPredictFun(X_test, w2)#predict y against whole testing data set\n",
        "          #print(y_prediction.shape)\n",
        "          er = y_prediction - y[(j*m):((j+1)*m)] #use prediction and true y to find error\n",
        "          g = np.matmul(np.transpose(X[(j*m):((j+1)*m)]),er)/float(m) #calculate gradient\n",
        "          w2 = w2 - learning_rate*g #update weights\n",
        "           \n",
        "          losses_tr = np.sqrt(np.square(np.subtract(y, y_prediction_all)).mean()) #current training error\n",
        "          losses_test = np.sqrt(np.square(np.subtract(y_test, ytest_prediction_all)).mean())#current testing error\n",
        "          loss.append(losses_tr) #array of training errors\n",
        "          loss_test.append(losses_test)#array of testing errors\n",
        "          iter = iter+1 #count number of iterations for each epoch\n",
        "          epochs.append(iter) #add to epoch array (used for plotting)\n",
        "          \n",
        "          plt.plot(epochs, loss, 'g', label= 'Training Error')\n",
        "          plt.plot(epochs,loss_test,'b', label= 'Testing Error')\n",
        "          plt.title('Cost Function')\n",
        "          plt.xlabel('Batches per Epoch Updates')\n",
        "          plt.ylabel('Loss')\n",
        "          #plt.legend()\n",
        "          #plt.show()\n",
        "          \n",
        "        else: #last batch in epoch\n",
        "          y_prediction = myPredictFun(X[(j*m):((j*m)+ training_leftover)],w2) #same as above but for leftover training data\n",
        "          y_prediction_all = myPredictFun(X,w2)\n",
        "          ytest_prediction_all = myPredictFun(X_test, w2)\n",
        "\n",
        "          er = y_prediction - y[(j*m):((j*m)+ training_leftover)] #use prediction and true y to find error\n",
        "          g = np.matmul(np.transpose(X[(j*m):((j*m)+ training_leftover)]),er)/float(training_leftover) #calculate gradient\n",
        "          w2 = w2 - learning_rate*g #update weights\n",
        "\n",
        "          losses_tr = np.sqrt(np.square(np.subtract(y, y_prediction_all)).mean())\n",
        "          losses_test = np.sqrt(np.square(np.subtract(y_test, ytest_prediction_all)).mean())\n",
        "          loss.append(losses_tr)\n",
        "          loss_test.append(losses_test)\n",
        "          iter = iter+1\n",
        "          epochs.append(iter)\n",
        "          \n",
        "          plt.plot(epochs, loss, 'g', label= 'Training Error')\n",
        "          plt.plot(epochs,loss_test,'b', label= 'Testing Error')\n",
        "          plt.title('Cost Function')\n",
        "          plt.xlabel('Batches per Epoch Updates')\n",
        "          plt.ylabel('Loss')\n",
        "          #plt.legend()\n",
        "          #plt.show()\n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        " \n",
        "    \n",
        "    return w2, loss, loss_test, epochs\n",
        "\n",
        "# Task 4-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return np.matmul(X,w) #dot product of weights and training data\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "#print(train_data_intercept.shape[1])\n",
        "train_size = train_data_intercept.shape[0]\n",
        "#print(test_data_intercept.shape[1])\n",
        "#print ('Sizes: ')\n",
        "test_size = test_data_intercept.shape[0]\n",
        "train_last_batch = train_size % 15\n",
        "test_last_batch = test_size % 15\n",
        "train_batches = int(train_size / 15) + 1\n",
        "test_batches = int(test_size/ 15) + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Here we call myGradientDescentFun to train the model and get weights\n",
        "# Note: you need to figure out good learning rate value and the number of iterations\n",
        "w, loss, loss_test, epochs = myGradientDescentFun(train_data_intercept,train_target,.05,10,15,test_data_intercept, test_target)\n",
        "#print(w)\n",
        "#print(loss[0:5])\n",
        "\n",
        "# Task 4-3: show intercept and coefficents\n",
        "print('intercept:')\n",
        "print(w[0])\n",
        "print('coefficients')\n",
        "print(w[1:len(w)-1])\n",
        "\n",
        "# Task 4-4: show errors on training dataset and testing dataset\n",
        "train_prediction = myPredictFun(train_data_intercept, w) #training prediction of final weights\n",
        "print('Training Error:')\n",
        "print(np.sqrt(np.square(np.subtract(train_target, train_prediction)).mean())) #training error\n",
        "\n",
        "test_prediction = myPredictFun(test_data_intercept, w)#testing prediction of final weights\n",
        "print('Testing Error:')\n",
        "print(np.sqrt(np.square(np.subtract(test_target, test_prediction)).mean())) #testing error\n",
        "\n",
        "# Task 4-5: plot learning curves showing training errors and testing errors along bath\n",
        "plt.plot(epochs,loss,'g',label = 'Training Error')\n",
        "plt.plot(epochs, loss_test, 'b', label = 'Testing Error' )\n",
        "plt.title('Cost Function')\n",
        "plt.xlabel('Batches per Epoch Updates')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}